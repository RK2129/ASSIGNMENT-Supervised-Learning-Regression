{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb0g18GJZN1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***ASSIGNMENT Supervised Learning: Regression***"
      ],
      "metadata": {
        "id": "rvKCt9RkZOUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ef3ZVl5HaiIA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nR-i_tJjZSSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Question 1: What is Simple Linear Regression (SLR)? Explain its purpose.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Simple Linear Regression (SLR)** is one of the most basic and commonly used techniques in machine learning and statistics.\n",
        "2. It is used to study and model the relationship between **two variables** — one **independent variable (X)** and one **dependent variable (Y)**.\n",
        "3. The main idea of SLR is to find a **straight-line relationship** between these two variables so that we can **predict** the value of Y for any given value of X.\n",
        "4. The relationship is expressed by the **equation of a straight line**:\n",
        "   [\n",
        "   Y = b₀ + b₁X + ε\n",
        "   ]\n",
        "   where,\n",
        "\n",
        "   * **Y** = Dependent variable (the value we want to predict)\n",
        "   * **X** = Independent variable (the predictor)\n",
        "   * **b₀** = Intercept (value of Y when X = 0)\n",
        "   * **b₁** = Slope (how much Y changes for a one-unit change in X)\n",
        "   * **ε (epsilon)** = Random error term (difference between actual and predicted value)\n",
        "5. The slope (**b₁**) tells us the **strength and direction** of the relationship between X and Y.\n",
        "\n",
        "   * If b₁ is **positive**, Y increases as X increases.\n",
        "   * If b₁ is **negative**, Y decreases as X increases.\n",
        "6. The intercept (**b₀**) shows the starting point of the line on the Y-axis when X = 0.\n",
        "7. The goal of SLR is to **fit the best possible line** through the data points so that the difference between the actual and predicted values is as small as possible.\n",
        "8. The best-fitting line is found using a method called the **“Least Squares Method”**, which minimizes the sum of squared errors.\n",
        "9. **Purpose of SLR:**\n",
        "\n",
        "   * To **predict** the dependent variable using a known value of the independent variable.\n",
        "   * To **analyze relationships** and understand how one factor affects another.\n",
        "   * To **make decisions** based on data patterns and trends.\n",
        "10. **Example:** Predicting a person’s salary (Y) based on their years of experience (X) using simple linear regression.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W27ikCXhZVMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 2: What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. To use **Simple Linear Regression (SLR)** correctly, some important **assumptions** must be satisfied.\n",
        "2. These assumptions ensure that the model gives **accurate, unbiased, and meaningful results**.\n",
        "3. If these assumptions are violated, the predictions and conclusions of the regression model may not be reliable.\n",
        "4. The key assumptions of Simple Linear Regression are explained below:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Linearity**\n",
        "\n",
        "5. The relationship between the independent variable (X) and the dependent variable (Y) must be **linear**.\n",
        "6. This means that a change in X produces a **proportional change** in Y.\n",
        "7. In other words, the data points should roughly form a **straight-line pattern** when plotted on a graph.\n",
        "8. If the relationship is curved or nonlinear, then simple linear regression is not suitable.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Independence of Errors**\n",
        "\n",
        "9. The **residuals (errors)** — which are the differences between the actual and predicted values — should be **independent** of each other.\n",
        "10. This means that the error for one observation should not influence the error for another.\n",
        "11. If errors are related (for example, in time-series data), it violates this assumption and can lead to wrong results.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Homoscedasticity (Equal Variance)**\n",
        "\n",
        "12. The variance of the residuals should be **constant across all levels of X**.\n",
        "13. This means that the spread of errors should be roughly the same whether X is small or large.\n",
        "14. If the errors increase or decrease systematically with X (called **heteroscedasticity**), it can affect the reliability of the model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Normality of Errors**\n",
        "\n",
        "15. The residuals (errors) should be **normally distributed**.\n",
        "16. This assumption is important for **hypothesis testing** and for constructing **confidence intervals**.\n",
        "17. It means that most errors are small, and very large errors (positive or negative) are rare.\n",
        "18. This can be checked using a **histogram** or **Q-Q plot** of residuals.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. No or Minimal Multicollinearity (for multiple regression only)**\n",
        "\n",
        "19. Although Simple Linear Regression has only one independent variable, in multiple regression this assumption ensures that **independent variables are not highly correlated** with each other.\n",
        "20. In SLR, this is automatically satisfied since there’s only one X variable.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. No Autocorrelation (especially in time series data)**\n",
        "\n",
        "21. The residuals should not show any systematic pattern over time.\n",
        "22. Autocorrelation happens when errors follow a trend or pattern, meaning the model missed some information.\n",
        "23. This can be checked using the **Durbin-Watson test**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary:**\n",
        "\n",
        "24. In short, the main assumptions are:\n",
        "\n",
        "* Linearity\n",
        "* Independence of errors\n",
        "* Equal variance (Homoscedasticity)\n",
        "* Normality of errors\n",
        "\n",
        "25. When these assumptions hold true, the linear regression model gives **valid, accurate, and trustworthy results**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mdztxCq5ZpvM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGiMlBpJZ47n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 3: Write the mathematical equation for a simple linear regression model and explain each term.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. The mathematical form of a **Simple Linear Regression (SLR)** model is:\n",
        "\n",
        "   [\n",
        "   Y = b₀ + b₁X + ε\n",
        "   ]\n",
        "\n",
        "2. This is the **equation of a straight line**, where the relationship between the dependent variable (Y) and the independent variable (X) is represented mathematically.\n",
        "\n",
        "3. Let’s understand each term in detail:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Y — Dependent Variable**\n",
        "\n",
        "4. Y is the **output variable** that we want to predict or explain.\n",
        "5. It depends on the changes in X.\n",
        "6. Example: In predicting house price, Y = Price of the house.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. X — Independent Variable**\n",
        "\n",
        "7. X is the **input variable**, also called the **predictor** or **explanatory variable**.\n",
        "8. It is the variable we use to predict Y.\n",
        "9. Example: In the house price example, X = Size of the house (in square feet).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. b₀ — Intercept (Constant Term)**\n",
        "\n",
        "10. b₀ is the **Y-intercept**, the point where the regression line crosses the Y-axis.\n",
        "11. It represents the **predicted value of Y when X = 0**.\n",
        "12. In other words, even if X is zero, b₀ gives us the starting or base value of Y.\n",
        "13. Example: If b₀ = 50, then when X = 0, the predicted value of Y = 50.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. b₁ — Slope (Regression Coefficient)**\n",
        "\n",
        "14. b₁ is the **slope of the regression line**.\n",
        "15. It tells us **how much Y changes** when X increases by **one unit**.\n",
        "16. A **positive b₁** means that Y increases as X increases (direct relationship).\n",
        "17. A **negative b₁** means that Y decreases as X increases (inverse relationship).\n",
        "18. Example: If b₁ = 2, it means for every 1-unit increase in X, Y increases by 2 units.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. ε (epsilon) — Error Term or Residual**\n",
        "\n",
        "19. ε represents the **error term** — the difference between the **actual** value of Y and the **predicted** value of Y.\n",
        "20. It accounts for the variation in Y that cannot be explained by X.\n",
        "21. Mathematically,\n",
        "    [\n",
        "    ε = Y_{actual} - Y_{predicted}\n",
        "    ]\n",
        "22. These errors are assumed to have an average value of zero and to be normally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Putting it All Together**\n",
        "\n",
        "23. The full equation can be read as:\n",
        "    “Predicted Y equals intercept plus slope times X, plus random error.”\n",
        "24. Each part of the equation has a role:\n",
        "\n",
        "* **b₀** shifts the line up or down.\n",
        "* **b₁** controls the tilt or angle of the line.\n",
        "* **ε** captures noise and randomness.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Example Calculation**\n",
        "\n",
        "25. Suppose the regression equation is\n",
        "    [\n",
        "    Y = 10 + 3X\n",
        "    ]\n",
        "\n",
        "* Here, **b₀ = 10** and **b₁ = 3**.\n",
        "\n",
        "26. If X = 5, then predicted Y = 10 + (3 × 5) = **25**.\n",
        "27. This means when X increases by 1 unit, Y increases by 3 units.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary**\n",
        "\n",
        "28. The **Simple Linear Regression equation** gives a mathematical way to describe how one variable changes with another.\n",
        "29. It helps in **prediction, forecasting, and understanding cause-and-effect relationships** between variables.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fX3xxpmYaF4a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYCwtpKhaLMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 4: Provide a real-world example where simple linear regression can be applied.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Simple Linear Regression (SLR)** is widely used in real-life situations where we want to predict one value based on another.\n",
        "2. It helps businesses, researchers, and organizations understand **how one factor affects another** and to make **data-driven decisions**.\n",
        "3. Let’s look at some **real-world examples**, and then we’ll explain one in detail.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Examples of where SLR can be applied:**\n",
        "\n",
        "4. Predicting a student’s **exam score** based on **hours studied**.\n",
        "5. Estimating a person’s **salary** based on **years of experience**.\n",
        "6. Forecasting a company’s **sales** based on **advertising expenditure**.\n",
        "7. Predicting **house prices** based on **area (square feet)**.\n",
        "8. Estimating **crop yield** based on **rainfall** or **fertilizer used**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Detailed Example: Salary Prediction Based on Experience**\n",
        "\n",
        "9. Suppose a company wants to predict an employee’s **salary** (Y) using their **years of experience** (X).\n",
        "10. The company collects data from several employees, noting each person’s experience and corresponding salary.\n",
        "11. The goal is to find a **linear relationship** between experience and salary — that is, to see if salary tends to increase as experience increases.\n",
        "12. The data might look like this:\n",
        "\n",
        "| Experience (Years) | Salary (in ₹ Lakh/year) |\n",
        "| ------------------ | ----------------------- |\n",
        "| 1                  | 3.0                     |\n",
        "| 2                  | 3.8                     |\n",
        "| 3                  | 4.5                     |\n",
        "| 4                  | 5.2                     |\n",
        "| 5                  | 6.0                     |\n",
        "\n",
        "13. When we plot this data on a graph, it roughly forms a straight line, showing that **salary increases with experience**.\n",
        "14. Using the **simple linear regression model**, we can fit an equation of the form:\n",
        "\n",
        "[\n",
        "Salary = b₀ + b₁ × Experience\n",
        "]\n",
        "\n",
        "15. Suppose we find the regression equation to be:\n",
        "    [\n",
        "    Salary = 2.5 + 0.7 × Experience\n",
        "    ]\n",
        "\n",
        "* Here, **b₀ = 2.5** (intercept): Base salary when experience = 0.\n",
        "* **b₁ = 0.7** (slope): Salary increases by ₹0.7 lakh for each extra year of experience.\n",
        "\n",
        "16. Using this equation, we can **predict future salaries**.\n",
        "\n",
        "* For example, if a person has 6 years of experience,\n",
        "  [\n",
        "  Salary = 2.5 + 0.7 × 6 = 6.7 \\text{ lakh per year.}\n",
        "  ]\n",
        "\n",
        "17. This helps the HR department estimate fair pay for new employees or plan salary budgets.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why Simple Linear Regression is Useful Here:**\n",
        "\n",
        "18. It shows a **clear quantitative relationship** between experience and salary.\n",
        "19. It allows easy **interpretation** — the slope tells exactly how much salary increases per year of experience.\n",
        "20. It helps in **decision-making**, **forecasting**, and **policy planning**.\n",
        "21. The model can also be improved later by adding more variables (like education, job role, or skills) for better predictions.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Other Real-Life Examples (Briefly):**\n",
        "\n",
        "22. **Agriculture:** Predicting crop yield based on rainfall or fertilizer quantity.\n",
        "23. **Economics:** Forecasting GDP based on government spending.\n",
        "24. **Health:** Predicting a person’s weight based on calorie intake or exercise hours.\n",
        "25. **Education:** Estimating student performance based on study hours or attendance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary:**\n",
        "\n",
        "26. Simple Linear Regression is very useful for **predicting continuous values** when there is a **linear relationship** between two variables.\n",
        "27. In real-world data, it provides a simple and effective way to analyze trends and make forecasts.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "riW8YPv8akZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 5: What is the method of least squares in linear regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. The **Method of Least Squares** is a mathematical technique used to find the **best-fitting line** through a set of data points in **Simple Linear Regression**.\n",
        "2. Its main goal is to minimize the **difference** between the actual data points and the values predicted by the regression line.\n",
        "3. These differences are called **errors** or **residuals**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Basic Idea**\n",
        "\n",
        "4. Suppose we have many data points plotted on a graph — each point shows an actual pair of values (X, Y).\n",
        "5. The regression line tries to pass as close as possible to all these points.\n",
        "6. However, not all points will lie exactly on the line — there will always be **some error** (difference between actual Y and predicted Y).\n",
        "7. The **Method of Least Squares** chooses the line that makes these errors as **small as possible**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Mathematical Explanation**\n",
        "\n",
        "8. Let’s say we have ( n ) observations:\n",
        "   [\n",
        "   (X₁, Y₁), (X₂, Y₂), (X₃, Y₃), ..., (X_n, Y_n)\n",
        "   ]\n",
        "9. The equation of the regression line is:\n",
        "   [\n",
        "   Ŷ = b₀ + b₁X\n",
        "   ]\n",
        "   where **Ŷ** is the predicted value of Y for a given X.\n",
        "10. The **residual (error)** for each observation is:\n",
        "    [\n",
        "    e_i = Y_i - Ŷ_i\n",
        "    ]\n",
        "11. The goal is to minimize the **sum of squared residuals**:\n",
        "    [\n",
        "    \\text{Minimize } S = \\sum (Y_i - Ŷ_i)^2 = \\sum (Y_i - (b₀ + b₁X_i))^2\n",
        "    ]\n",
        "12. We square the errors because:\n",
        "\n",
        "    * Squaring makes all errors positive (since some may be negative).\n",
        "    * It gives more weight to larger errors.\n",
        "13. By minimizing this sum (S), we find the **best values of b₀ and b₁** that make the line fit the data most accurately.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Deriving b₀ and b₁ (Formulas)**\n",
        "\n",
        "14. Using calculus, we take the partial derivatives of S with respect to b₀ and b₁, and set them to zero to find the minimum point.\n",
        "15. After simplification, we get the formulas:\n",
        "\n",
        "[\n",
        "b₁ = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "]\n",
        "[\n",
        "b₀ = \\bar{Y} - b₁\\bar{X}\n",
        "]\n",
        "where:\n",
        "\n",
        "* (\\bar{X}) = mean of X values\n",
        "* (\\bar{Y}) = mean of Y values\n",
        "\n",
        "16. These formulas give the **slope (b₁)** and **intercept (b₀)** of the best-fitting regression line.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Why It’s Called “Least Squares”**\n",
        "\n",
        "17. It is called the “Least Squares” method because it **minimizes the sum of the squares** of the residuals (errors).\n",
        "18. In other words, out of all possible lines, it finds the one with the **least total squared error**.\n",
        "19. The smaller the total squared error, the better the line fits the data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Example**\n",
        "\n",
        "20. Suppose we have the following data for study hours (X) and test scores (Y):\n",
        "\n",
        "| Hours Studied (X) | Score (Y) |\n",
        "| ----------------- | --------- |\n",
        "| 2                 | 81        |\n",
        "| 3                 | 85        |\n",
        "| 4                 | 88        |\n",
        "| 5                 | 92        |\n",
        "\n",
        "21. Using the method of least squares, we can calculate values of **b₀ and b₁** that produce the best-fitting line:\n",
        "    [\n",
        "    Y = b₀ + b₁X\n",
        "    ]\n",
        "    For example, if we find ( b₀ = 78 ) and ( b₁ = 3.2 ),\n",
        "    then the regression equation becomes:\n",
        "    [\n",
        "    Y = 78 + 3.2X\n",
        "    ]\n",
        "22. This means each extra hour of study increases the score by about **3.2 marks**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Importance of the Least Squares Method**\n",
        "\n",
        "23. It is the **foundation** of linear regression modeling.\n",
        "24. It ensures that the model gives **the best possible fit** to the data.\n",
        "25. It helps in making accurate **predictions** and understanding relationships between variables.\n",
        "26. It is also **computationally simple and widely used** in machine learning, statistics, and data science.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary**\n",
        "\n",
        "27. The **Method of Least Squares** finds the regression line that minimizes the total squared differences between observed and predicted values.\n",
        "28. It provides the most accurate line of best fit, ensuring reliable predictions and analysis.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Iknim06ha9jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 6: What is Logistic Regression? How does it differ from Linear Regression?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Introduction**\n",
        "\n",
        "1. **Logistic Regression** is a type of **supervised learning algorithm** used for **classification problems**, not for predicting continuous values.\n",
        "2. It is called “regression” because it uses a mathematical model similar to linear regression, but it is used to predict **categorical outcomes** — like **Yes/No**, **True/False**, **0/1**, or **Pass/Fail**.\n",
        "3. The main purpose of logistic regression is to estimate the **probability** that a given input belongs to a particular class.\n",
        "4. For example, it can predict whether an email is **spam (1)** or **not spam (0)**, or whether a student will **pass (1)** or **fail (0)** an exam based on their study hours.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Why Linear Regression Cannot Be Used for Classification**\n",
        "\n",
        "5. Linear regression predicts **continuous values** (like house prices or salaries).\n",
        "6. However, classification problems need outputs between **0 and 1**, representing probabilities.\n",
        "7. If we use linear regression for such problems, predictions might go beyond this range (e.g., -0.5 or 1.3), which is **not valid for probabilities**.\n",
        "8. Logistic regression solves this problem by using a **special function** that converts any real number into a value between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. The Logistic (Sigmoid) Function**\n",
        "\n",
        "9. Logistic regression uses a **Sigmoid function**, also called the **logistic function**, to transform the output.\n",
        "10. The formula for the sigmoid function is:\n",
        "    [\n",
        "    P(Y = 1|X) = \\frac{1}{1 + e^{-(b₀ + b₁X)}}\n",
        "    ]\n",
        "11. Here,\n",
        "\n",
        "    * **P(Y = 1|X)** is the probability that Y equals 1 given X.\n",
        "    * **e** is the base of the natural logarithm (≈ 2.718).\n",
        "    * **b₀** = intercept and **b₁** = coefficient (like in linear regression).\n",
        "12. The sigmoid function “squeezes” any value of ( b₀ + b₁X ) into the range **(0, 1)**.\n",
        "13. If the output is greater than 0.5, we classify it as **1** (positive class).\n",
        "14. If it’s less than 0.5, we classify it as **0** (negative class).\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Mathematical Equation**\n",
        "\n",
        "15. The logistic regression equation looks similar to linear regression:\n",
        "    [\n",
        "    \\text{Logit}(P) = \\ln\\left(\\frac{P}{1 - P}\\right) = b₀ + b₁X\n",
        "    ]\n",
        "16. Here,\n",
        "\n",
        "    * **P** = Probability of success (Y = 1).\n",
        "    * **1 - P** = Probability of failure (Y = 0).\n",
        "    * **ln(P / (1 - P))** is called the **log-odds** or **logit**.\n",
        "17. So logistic regression doesn’t predict Y directly — it predicts the **log-odds** of Y and converts them into probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Example**\n",
        "\n",
        "18. Suppose we want to predict whether a student will **pass** (1) or **fail** (0) an exam based on **hours studied**.\n",
        "19. The logistic regression model might look like this:\n",
        "    [\n",
        "    P(\\text{Pass}) = \\frac{1}{1 + e^{-( -3 + 1.2X )}}\n",
        "    ]\n",
        "20. If a student studies **2 hours**,\n",
        "    [\n",
        "    P(\\text{Pass}) = \\frac{1}{1 + e^{-(-3 + 2.4)}} = 0.31\n",
        "    ]\n",
        "    → So, there is a **31% chance** of passing.\n",
        "21. If a student studies **5 hours**,\n",
        "    [\n",
        "    P(\\text{Pass}) = \\frac{1}{1 + e^{-(-3 + 6)}} = 0.95\n",
        "    ]\n",
        "    → So, there is a **95% chance** of passing.\n",
        "22. Hence, logistic regression gives a probability-based prediction instead of a continuous number.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Difference Between Linear and Logistic Regression**\n",
        "\n",
        "| **Aspect**               | **Linear Regression**                   | **Logistic Regression**                         |\n",
        "| ------------------------ | --------------------------------------- | ----------------------------------------------- |\n",
        "| **Type of Output**       | Continuous (e.g., salary, price)        | Categorical (e.g., Yes/No, 0/1)                 |\n",
        "| **Goal**                 | Predicts actual numerical value         | Predicts probability of belonging to a class    |\n",
        "| **Equation**             | ( Y = b₀ + b₁X )                        | ( P = \\frac{1}{1 + e^{-(b₀ + b₁X)}} )           |\n",
        "| **Error Measurement**    | Measured using Mean Squared Error (MSE) | Measured using Log Loss or Cross-Entropy        |\n",
        "| **Linearity Assumption** | Assumes Y is linearly related to X      | Assumes log-odds of Y are linearly related to X |\n",
        "| **Output Range**         | From -∞ to +∞                           | Always between 0 and 1                          |\n",
        "| **Use Case**             | Regression (predicting quantity)        | Classification (predicting category)            |\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Summary**\n",
        "\n",
        "23. **Logistic Regression** is a classification algorithm that predicts **probabilities** of different outcomes.\n",
        "24. It uses the **sigmoid function** to ensure outputs are between 0 and 1.\n",
        "25. **Linear Regression** predicts **continuous values**, while **Logistic Regression** predicts **class probabilities**.\n",
        "26. Both are foundational models in **machine learning** and are used as a starting point before moving to more complex algorithms.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4xVd7XsNbSPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Question 7: Name and briefly describe three common evaluation metrics for regression models.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Introduction**\n",
        "\n",
        "1. After building a regression model, it is important to measure **how well the model performs**.\n",
        "2. Evaluation metrics help us understand **how accurate** our predictions are compared to the actual data.\n",
        "3. In regression, the outputs are **continuous values** (like price, salary, temperature, etc.), so we use specific metrics that measure **error or deviation** between predicted and true values.\n",
        "4. The three most common evaluation metrics used for regression models are:\n",
        "\n",
        "   * **Mean Absolute Error (MAE)**\n",
        "   * **Mean Squared Error (MSE)**\n",
        "   * **Root Mean Squared Error (RMSE)**\n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ Mean Absolute Error (MAE)**\n",
        "\n",
        "5. **Definition:**\n",
        "   The Mean Absolute Error measures the **average absolute difference** between the actual values and the predicted values.\n",
        "6. It tells us, on average, how much the model’s predictions differ from the real results.\n",
        "7. The mathematical formula for MAE is:\n",
        "   [\n",
        "   MAE = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i|\n",
        "   ]\n",
        "   where,\n",
        "\n",
        "   * ( Y_i ) = actual value,\n",
        "   * ( \\hat{Y}_i ) = predicted value,\n",
        "   * ( n ) = total number of observations.\n",
        "8. The **absolute value** (| |) ensures that errors are treated as positive numbers.\n",
        "9. **Interpretation:**\n",
        "\n",
        "   * A **lower MAE** means the model’s predictions are **closer** to the actual values.\n",
        "   * It gives a **direct and easy-to-understand** measure of average prediction error.\n",
        "10. **Example:**\n",
        "    If MAE = 5, it means that, on average, the model’s predictions are off by **5 units**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Mean Squared Error (MSE)**\n",
        "\n",
        "11. **Definition:**\n",
        "    The Mean Squared Error measures the **average of the squared differences** between actual and predicted values.\n",
        "12. The formula for MSE is:\n",
        "    [\n",
        "    MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
        "    ]\n",
        "13. Here, we **square the errors** to penalize larger errors more than smaller ones.\n",
        "14. This means MSE is more sensitive to **outliers** (large mistakes) in the data.\n",
        "15. **Interpretation:**\n",
        "\n",
        "    * A **smaller MSE** indicates a better model fit.\n",
        "    * However, because the errors are squared, the value of MSE is in **squared units** of the target variable.\n",
        "16. **Example:**\n",
        "    If we are predicting house prices (in ₹), and MSE = 4,000, it means the average squared error is ₹4,000² — but not easy to interpret directly.\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Root Mean Squared Error (RMSE)**\n",
        "\n",
        "17. **Definition:**\n",
        "    The Root Mean Squared Error is simply the **square root of MSE**.\n",
        "18. The formula for RMSE is:\n",
        "    [\n",
        "    RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 }\n",
        "    ]\n",
        "19. It gives an error value in the **same units** as the dependent variable, making it easier to interpret.\n",
        "20. **Interpretation:**\n",
        "\n",
        "    * RMSE represents the **standard deviation of the prediction errors**.\n",
        "    * A **lower RMSE** means the model is performing better.\n",
        "    * RMSE gives more weight to large errors than MAE, so it’s useful when you want to heavily penalize large mistakes.\n",
        "21. **Example:**\n",
        "    If RMSE = 6, it means the model’s predictions are, on average, **6 units away** from the actual values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of the Three Metrics**\n",
        "\n",
        "| **Metric** | **Formula**                                     | **Meaning**               | **Advantages**                            | **Disadvantages**             |                              |                                       |\n",
        "| ---------- | ----------------------------------------------- | ------------------------- | ----------------------------------------- | ----------------------------- | ---------------------------- | ------------------------------------- |\n",
        "| **MAE**    | ( \\frac{1}{n} \\sum                              | Y_i - \\hat{Y}_i           | )                                         | Average of absolute errors    | Simple and easy to interpret | Doesn’t penalize large errors heavily |\n",
        "| **MSE**    | ( \\frac{1}{n} \\sum (Y_i - \\hat{Y}_i)^2 )        | Average of squared errors | Penalizes large errors                    | Not in same unit as Y         |                              |                                       |\n",
        "| **RMSE**   | ( \\sqrt{\\frac{1}{n} \\sum (Y_i - \\hat{Y}_i)^2} ) | Square root of MSE        | Same unit as Y, sensitive to large errors | Can be influenced by outliers |                              |                                       |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Summary**\n",
        "\n",
        "22. **MAE**, **MSE**, and **RMSE** are the most common and reliable metrics for evaluating regression models.\n",
        "23. **MAE** gives a simple average error, **MSE** gives more weight to larger errors, and **RMSE** provides an interpretable measure in the same unit as the target variable.\n",
        "24. Choosing the right metric depends on the specific problem and whether we want to **penalize large errors** more heavily or not.\n",
        "25. Together, these metrics help us determine how accurate and effective our regression model is.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AyJV5X8TbpbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Question 8: What is the purpose of the R-squared metric in regression analysis?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Introduction**\n",
        "\n",
        "1. **R-squared (R²)**, also called the **Coefficient of Determination**, is one of the most important metrics in regression analysis.\n",
        "2. It measures how well the **independent variable(s)** explain the **variation** in the dependent variable.\n",
        "3. In simple words, R² tells us **how well our regression model fits the data**.\n",
        "4. The value of R² always lies between **0 and 1**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Meaning of R-squared**\n",
        "\n",
        "5. R² represents the **proportion (percentage)** of the variation in the dependent variable (Y) that can be explained by the independent variable (X).\n",
        "6. If R² = 0.8 (or 80%), it means that **80% of the variation** in Y is explained by X — and the remaining 20% is due to other unknown factors or random error.\n",
        "7. A **higher R² value** means the model explains more of the data’s variation, indicating a better fit.\n",
        "8. A **lower R² value** means the model explains less variation, meaning it doesn’t fit the data well.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Mathematical Formula**\n",
        "\n",
        "9. The formula for R² is:\n",
        "   [\n",
        "   R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
        "   ]\n",
        "   where,\n",
        "\n",
        "   * ( SS_{res} = \\sum (Y_i - \\hat{Y}_i)^2 ) → **Residual Sum of Squares** (unexplained variation)\n",
        "   * ( SS_{tot} = \\sum (Y_i - \\bar{Y})^2 ) → **Total Sum of Squares** (total variation in data)\n",
        "10. The ratio ( \\frac{SS_{res}}{SS_{tot}} ) shows how much variation is **not explained** by the model.\n",
        "11. Subtracting it from 1 gives the proportion of variation **explained** by the model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Example**\n",
        "\n",
        "12. Suppose we are predicting **students’ marks (Y)** based on **hours studied (X)**.\n",
        "13. After fitting a linear regression model, we calculate:\n",
        "\n",
        "* ( SS_{tot} = 200 ) (total variation in marks)\n",
        "* ( SS_{res} = 40 ) (variation not explained by the model)\n",
        "\n",
        "14. Then,\n",
        "    [\n",
        "    R^2 = 1 - \\frac{40}{200} = 1 - 0.2 = 0.8\n",
        "    ]\n",
        "15. This means that **80% of the variation** in students’ marks can be explained by study hours.\n",
        "16. The remaining 20% is due to other factors like teaching quality, sleep, or motivation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Interpretation of R-squared Values**\n",
        "\n",
        "17. **R² = 0:** The model explains **none** of the variation in Y. Predictions are useless.\n",
        "18. **R² = 1:** The model explains **all** the variation perfectly (a perfect fit).\n",
        "19. **0 < R² < 1:** The model explains some, but not all, of the variation in Y.\n",
        "20. Generally, a **higher R²** is better, but it doesn’t always mean the model is perfect.\n",
        "21. Sometimes, an **R² that is too high** (close to 1) could mean **overfitting** — the model fits the training data too well but fails on new data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Purpose and Importance**\n",
        "\n",
        "22. R-squared is used to measure the **goodness of fit** of a regression model.\n",
        "23. It helps us understand **how much of the dependent variable’s behavior** can be predicted by the independent variable.\n",
        "24. It is a **summary statistic** — a quick way to see whether your model is useful.\n",
        "25. It can also be compared across models to find **which one performs better**.\n",
        "26. However, R² alone is not enough — it should be used along with other metrics like **MAE, MSE, RMSE**, and **Adjusted R²** (especially in multiple regression).\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Example Interpretation Table**\n",
        "\n",
        "| **R² Value** | **Interpretation**                 |\n",
        "| ------------ | ---------------------------------- |\n",
        "| 0.0 – 0.3    | Weak relationship (poor model fit) |\n",
        "| 0.3 – 0.6    | Moderate relationship              |\n",
        "| 0.6 – 0.9    | Strong relationship                |\n",
        "| 0.9 – 1.0    | Very strong (possibly overfitted)  |\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Summary**\n",
        "\n",
        "27. **R-squared** tells us how well the regression model explains the variability of the dependent variable.\n",
        "28. A **higher R²** means a better-fitting model.\n",
        "29. However, it should always be interpreted with caution — a high R² does not always mean the model is correct or useful.\n",
        "30. In summary, R² is a **measure of model accuracy and explanatory power** in regression analysis.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Fo9lgfVbuxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **THEORY (Explanation Only)**\n",
        "\n",
        "1. **Simple Linear Regression (SLR)** is a supervised learning technique used to find the **linear relationship** between two variables — one **independent variable (X)** and one **dependent variable (Y)**.\n",
        "\n",
        "2. Its main goal is to find the **best-fitting straight line** that predicts the value of Y from X.\n",
        "\n",
        "3. The mathematical form of the line is:\n",
        "   [\n",
        "   Y = b₀ + b₁X\n",
        "   ]\n",
        "   where:\n",
        "\n",
        "   * **b₀** → Intercept (value of Y when X = 0)\n",
        "   * **b₁** → Slope (change in Y for a one-unit change in X)\n",
        "\n",
        "4. In Python, the **scikit-learn (sklearn)** library provides a built-in class called **LinearRegression()** for implementing this model easily.\n",
        "\n",
        "5. The steps to perform linear regression using scikit-learn are as follows:\n",
        "\n",
        "   * Import the required libraries like `numpy` and `LinearRegression`.\n",
        "   * Prepare the data (arrays of X and Y).\n",
        "   * Create a LinearRegression model object.\n",
        "   * Train the model using the `.fit()` function.\n",
        "   * Retrieve the **slope** and **intercept** using `.coef_` and `.intercept_`.\n",
        "\n",
        "6. The **slope (b₁)** shows how much Y changes when X increases by one unit.\n",
        "\n",
        "   * If b₁ is **positive**, Y increases as X increases.\n",
        "   * If b₁ is **negative**, Y decreases as X increases.\n",
        "\n",
        "7. The **intercept (b₀)** is the base value of Y when X = 0, showing where the regression line crosses the Y-axis.\n",
        "\n",
        "8. The model uses the **Method of Least Squares** to minimize the difference between the **actual values (Y)** and **predicted values (Ŷ)**.\n",
        "\n",
        "9. Once the model is trained, it can be used to **predict future values** of Y for any given X using the `.predict()` function.\n",
        "\n",
        "10. Linear Regression is one of the most fundamental techniques in **machine learning** and is used for **forecasting, trend analysis, and prediction tasks**.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary**\n",
        "\n",
        "* Simple Linear Regression finds a straight-line relationship between X and Y.\n",
        "* **b₀** (intercept) and **b₁** (slope) define this line.\n",
        "* Scikit-learn makes it easy to calculate and interpret these coefficients.\n",
        "* It helps in **predicting continuous values** and understanding the relationship between variables.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vKdgYyfCcB3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create dataset\n",
        "X = np.array([[1], [2], [3], [4], [5]])   # Independent variable\n",
        "Y = np.array([2, 4, 5, 4, 5])             # Dependent variable\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Print slope and intercept\n",
        "print(\"Slope (b1):\", model.coef_[0])\n",
        "print(\"Intercept (b0):\", model.intercept_)\n",
        "\n",
        "# Predict values\n",
        "Y_pred = model.predict(X)\n",
        "print(\"Predicted Values:\", Y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0QlwFB6cSP4",
        "outputId": "1e06b909-7904-43ed-9dab-ea9982793bed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (b1): 0.6\n",
            "Intercept (b0): 2.2\n",
            "Predicted Values: [2.8 3.4 4.  4.6 5.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 10: How do you interpret the coefficients in a simple linear regression model?**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **THEORY (Explanation Only)**\n",
        "\n",
        "1. In a **Simple Linear Regression** model, the relationship between the dependent variable (**Y**) and the independent variable (**X**) is represented by the equation:\n",
        "   [\n",
        "   Y = b₀ + b₁X\n",
        "   ]\n",
        "   where:\n",
        "\n",
        "   * **b₀** = Intercept\n",
        "   * **b₁** = Slope (regression coefficient)\n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ Intercept (b₀)**\n",
        "\n",
        "2. The **intercept (b₀)** is the predicted value of **Y** when the independent variable **X = 0**.\n",
        "3. It represents the point where the regression line crosses the Y-axis.\n",
        "4. In simple terms, it is the **starting value** or **baseline value** of the dependent variable when there is no input from X.\n",
        "5. Example:\n",
        "\n",
        "   * Suppose the regression equation is\n",
        "     [\n",
        "     Y = 2.5 + 0.8X\n",
        "     ]\n",
        "\n",
        "     * Here, **b₀ = 2.5** means that when X = 0, the predicted value of Y is **2.5**.\n",
        "6. However, the intercept may not always have practical meaning — for example, if X = 0 is not realistic (like 0 years of experience), we just treat b₀ as a mathematical constant.\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Slope (b₁)**\n",
        "\n",
        "7. The **slope (b₁)**, also called the **regression coefficient**, measures the **change in Y** for a **one-unit change in X**.\n",
        "8. It tells us **how strongly and in what direction** the independent variable affects the dependent variable.\n",
        "9. If **b₁ is positive**, it means there is a **positive relationship** between X and Y — as X increases, Y also increases.\n",
        "10. If **b₁ is negative**, it means there is a **negative relationship** — as X increases, Y decreases.\n",
        "11. Example:\n",
        "\n",
        "    * Using the same equation\n",
        "      [\n",
        "      Y = 2.5 + 0.8X\n",
        "      ]\n",
        "\n",
        "      * **b₁ = 0.8** means that for every **1 unit increase in X**, Y increases by **0.8 units**.\n",
        "12. So, if X = 5, the predicted Y would be:\n",
        "    [\n",
        "    Y = 2.5 + 0.8(5) = 6.5\n",
        "    ]\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Practical Example**\n",
        "\n",
        "13. Suppose we are predicting **salary (Y)** based on **years of experience (X)** using this regression equation:\n",
        "    [\n",
        "    Salary = 30,000 + 5,000 × (Experience)\n",
        "    ]\n",
        "14. Here,\n",
        "\n",
        "    * **b₀ = 30,000:** Base salary when experience = 0 years.\n",
        "    * **b₁ = 5,000:** Salary increases by ₹5,000 for every additional year of experience.\n",
        "15. This tells us that the slope gives the **rate of change**, and the intercept gives the **starting level**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ General Interpretation**\n",
        "\n",
        "16. **b₀ (Intercept):**\n",
        "\n",
        "    * The expected value of Y when X = 0.\n",
        "    * Represents the starting point of the regression line.\n",
        "17. **b₁ (Slope):**\n",
        "\n",
        "    * The expected change in Y for a one-unit increase in X.\n",
        "    * Indicates the **direction and strength** of the relationship.\n",
        "18. Together, they form the regression equation that helps predict outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "### **5️⃣ Significance of Coefficients**\n",
        "\n",
        "19. Coefficients can also be tested statistically to see if they are **significant** (using hypothesis tests).\n",
        "20. A significant coefficient (p-value < 0.05) means that the variable **X** has a real impact on **Y**.\n",
        "21. The magnitude of **b₁** shows how strong the effect is, while the sign (+ or -) shows the direction.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary**\n",
        "\n",
        "22. In summary:\n",
        "\n",
        "* The **intercept (b₀)** shows where the line starts on the Y-axis.\n",
        "* The **slope (b₁)** shows how Y changes when X changes by one unit.\n",
        "\n",
        "23. These coefficients together explain the **relationship, strength, and direction** between the two variables.\n",
        "24. Interpreting them correctly helps in understanding how one variable influences another and in making accurate predictions.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Final takeaway:**\n",
        "The intercept (b₀) gives the baseline value, while the slope (b₁) gives the rate of change — together, they make regression analysis meaningful and interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bqy01ADSdUCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5AwIYLpcxZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}